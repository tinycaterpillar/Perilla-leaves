# 결과

![result](log/%EA%B2%B0%EA%B3%BC.png)
12등했다.

# 시행착오
성능은 획기적으로 올려준건 결측값의 비율이 너무 높은 데이터를 `train data`에서 제거하는 부분이었다.
![result](log/%ED%95%B5%EC%8B%AC.png)

그 외의 다른것들은 성능 향상에 크게 도움이 되지 않았다. 딥러닝 공부하는 친구가 데이터 전처리에만 집중하라고 했었는데 역시 선구자의 말은 세겨듣는게 맞다.

이번 대회에 참여하면서 그동안 `learning rate`를 몹시 간과했었다는걸 알게됐다. 학습이 안된다고 모델을 바로 폐기하려고 했는데 지인의 조언으로 `lr`을 조정하니 학습이 되는 마법을 경험했다.

또한 데이터 전처리에서 `Normalization`과 `standarization`을 적용해봤다. 왠지는 모르겠는데 `min-max normalization`은 성능이 굉장히 안좋게 나왔다. 데이터의 분산을 1, 평균을 0으로 만드는 `standarization`은 적용하지 않았을 때와 성능은 비슷했지만 loss가 더 빨리 떨어지는 것을 확인했다.

`lr scheduler`도 사용해봤다. 최근에는 `cosine`뭐시기를 많이 사용한다고 해서 적용해봤는데 `loss`값이 들쑥날쑥 튀어서 적용하기 어려웠다. 마지막에는 그냥 `lr`을 상수로 고정해서 사용했다.

`custom lossfunction`도 사용해봤다. 리더보드에서 채점할 때 사용하는 함수를 그대로 사용했다. 점수가 낮을 수록 상위기록이었고 학습이 lossfunction 을 줄이는 방향으로 진행되니까 괜찮은 방법이라 생각했다. 
근데 결론적으로는 도움이 되지 않았다.